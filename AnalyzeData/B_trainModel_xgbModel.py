import os
import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from collections import Counter
from scipy.stats import skew, kurtosis
from xgboost import XGBClassifier

# ==== é…ç½® ====
data_dir = 'signal_saved'
window_size = 100
step_size = 50
fs = 500

# ==== æ»¤æ³¢å™¨å‡½æ•° ====
# def bandpass_filter(signal, low=20, high=200, fs=500):
#     nyq = 0.5 * fs
#     high = min(high, nyq * 0.99)
#     b, a = butter(4, [low / nyq, high / nyq], btype='band')
#     return filtfilt(b, a, signal)

# ==== ç‰¹å¾æå– ====
def extract_features(signal):
    mav = np.mean(np.abs(signal))
    rms = np.sqrt(np.mean(signal ** 2))
    wl = np.sum(np.abs(np.diff(signal)))
    zc = np.sum(np.diff(np.sign(signal)) != 0)
    ssc = np.sum(np.diff(np.sign(np.diff(signal))) != 0)
    sk = skew(signal)
    ku = kurtosis(signal)
    return [mav, rms, wl, zc, ssc, sk, ku]

# ==== å•æ–‡ä»¶å¤„ç† ====
def process_file(filepath):
    df = pd.read_csv(filepath)
    signal = df['EMG Value'].values
    labels = df['Label'].values
    features, window_labels = [], []

    for start in range(0, len(signal) - window_size, step_size):
        win_signal = signal[start:start + window_size]
        win_label = labels[start + int(0.1*window_size):start + int(0.9*window_size)]
        feats = extract_features(win_signal)
        label = Counter(win_label).most_common(1)[0][0]
        features.append(feats)
        window_labels.append(label)

    return features, window_labels

# ==== è¯»å–æ•°æ® ====
all_features, all_labels = [], []
for file in os.listdir(data_dir):
    if file.endswith('.csv'):
        f_path = os.path.join(data_dir, file)
        print(f"ğŸ“‚ æ­£åœ¨å¤„ç†ï¼š{f_path}")
        feats, labels = process_file(f_path)
        all_features.extend(feats)
        all_labels.extend(labels)

X = np.array(all_features)
y_str = np.array(all_labels)
print(f"âœ… è¯»å–å®Œæ¯•ï¼šå…± {len(X)} ä¸ªçª—å£æ ·æœ¬")

# ==== æ ‡ç­¾ç¼–ç  ====
le = LabelEncoder()
y = le.fit_transform(y_str)
joblib.dump(le, 'label_encoder.pkl')  # ä¿å­˜ç¼–ç å™¨

# ==== è®­ç»ƒæ¨¡å‹ ====
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1,
                    use_label_encoder=False, eval_metric='logloss')
clf.fit(X_train, y_train)

# ==== è¯„ä¼° ====
y_pred = clf.predict(X_test)
print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))

cm = confusion_matrix(le.inverse_transform(y_test), le.inverse_transform(y_pred), labels=le.classes_)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix(XGBoost)")
plt.tight_layout()
plt.show()

# ==== ä¿å­˜æ¨¡å‹ ====
joblib.dump(clf, 'xgbModel.pkl')
print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º xgbModel.pkl")
